{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiteshShettyK90/AgenticDaprDemo/blob/main/Hitesh_Karunakara_Shetty_%5BStudent_Version%5D_Langchain_Week2_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is part of the course: [LLM Apps with Langchain](https://uplimit.com/course/llm-apps-with-langchain) and is created by Sidharth Ramachandran as the project for Week 2 of the course."
      ],
      "metadata": {
        "id": "q33EDjivuSqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's build \"InfoScout\" - a web research agent that prepares a detailed report for us on any topic and continuously adapts it based on feedback.\n",
        "\n",
        "<a href=\"https://ibb.co/MNk1LJh\"><img src=\"https://i.ibb.co/F74YtdW/DALL-E-2024-08-01-10-13-13-A-futuristic-agent-conducting-web-research-for-a-product-called-Info-Scou.webp\" alt=\"DALL-E-2024-08-01-10-13-13-A-futuristic-agent-conducting-web-research-for-a-product-called-Info-Scou\" border=\"0\"></a><br /><a target='_blank' href='https://de.imgbb.com/'></a><br />"
      ],
      "metadata": {
        "id": "5jgO63IiudPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We rely on web searches to perform a lot of tasks in our daily life. It might be mundane things like looking up addresses and movie show times or more involved like performing a market study or job research. Inspired by the likes of [Perplexity](https://www.perplexity.ai/) we would like to create our own personalized web research agent that executes these tasks for us. This can be extremely beneficial in terms of saving our time and probably find some interesting information that we might have otherwise missed out!\n",
        "\n",
        "In the process of building InfoScout, we will make use of the Langgraph framework that is part of the Langchain library."
      ],
      "metadata": {
        "id": "YaBE17yzvsOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üë®‚Äçüéì Learner Project\n",
        "\n",
        "Your goal in this project is to build a web research assistant that is capable of writing reports or articles based on research performed on the internet. We provide you with some basic scaffolding to create the agent but also suggest üìù `Learner Tasks` where you have to complete the prompt or code."
      ],
      "metadata": {
        "id": "YXuKS57lArqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0 - Pre-requisites"
      ],
      "metadata": {
        "id": "LFP6G0jC4vFM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE8lNU1buRkc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph==0.1.17 tavily-python langchainhub gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the main tools that we will use in building this agent is a web search engine. By default, LLMs do not have access to the internet and we need to provide them with this information. There are a lot of providers like Perplexity, SERP API and Tavily - that we will use in this project.\n",
        "\n",
        "Please sign-up for an account with [Tavily](https://tavily.com/) We will make use of Tavily and then navigate to the Overview page where you will find the API key. With a free account, you can make upto 1000 API requests per month and this should be more than sufficient for this project. Please add the TAVILY_API_KEY as another security key in the left tab of the Colab notebook as shown in the Week 1 project.\n",
        "\n",
        "<a href=\"https://ibb.co/1qLRGh4\"><img src=\"https://i.ibb.co/d2rk41H/Screenshot-2024-08-01-at-10-19-23.png\" alt=\"Screenshot-2024-08-01-at-10-19-23\" border=\"0\"></a><br /><a target='_blank' href='https://de.imgbb.com/'></a><br />\n",
        "\n",
        "If you have already completed the Week 1 project then your OPENAI_API_KEY would already be saved as part of the Colab environment, if not please take the necessary steps."
      ],
      "metadata": {
        "id": "MT_HB1qw42Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Initializing the individual components of our Agent graph\n",
        "\n",
        "The best way to think about agents is to break down the goal into sub-tasks and approach it similarly to how you would as a human being. With the critial difference being that each sub-task is performed by an LLM. For instance, let's consider how we can break down the goal of generating a research article -\n",
        "\n",
        "1. Depending on the topic, we would think about search terms or queries that will lead us to more information about the topic.\n",
        "2. We also have to actually execute those search queries - typically we would do that along with the first step as a human being but when working with LLMs, this can be treated as a seperate step.\n",
        "3. Based on the information we have gathered, we could generate our first draft document.\n",
        "4. Once you have gathered enough information, you would then like to get a review of the document from your colleagues or supervisor to make sure that you're going in the right direction.\n",
        "5. Finally, you would use the feedback you received to refine and iterate on your draft - making additional searches and research when necessary.\n",
        "\n",
        "In this project, we will translate this process into code with the help of Langraph - a library that is part of Langchain and allows us to define various flows of information. This can also be achieved by writing simple logical steps and loops but Langgraph provides us with additional functionality like state storage and more that makes it worthwhile to use it. It also makes it much more extensible as you will see during the project."
      ],
      "metadata": {
        "id": "ajbRZPKW58lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have identified the high-level workflow as outlined above. Let us start by defining the individual components. I found this to be an easier way of understanding how to make use of Langgraph. As we define each function, we will also be able to identify state variables that we will need and we can add that to the AgentState object. Let's see what we mean."
      ],
      "metadata": {
        "id": "Xkxs3aXT7tYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First step is to define the model that we are going to use for our workflow."
      ],
      "metadata": {
        "id": "WFqR3XzCqX2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "Vn0KbWsK4tm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's think about the first step in our identified workflow - the creation of search queries. We delegate this to an LLM using the prompt below."
      ],
      "metadata": {
        "id": "vTIbsJy0qh-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RESEARCH_PROMPT = \"\"\"You are an expert web researcher who understands how Google works and can come up with \\\n",
        "multiple search queries for a given task.\n",
        "\n",
        "You will be provided with the original user task or topic and your job is to identify the three best search queries \\\n",
        "that will retrieve the most information to complete that task. Your goal is to ensure different aspects of the task \\\n",
        "are covered and in the end we can provide a comprehensive overview to solve the problem. \\\n",
        "\n",
        "In some cases, you might be provided with additional tasks or aspects that the user wants to improve \\\n",
        "so please adapt your search queries based on this.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "F2ziK67W8CP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want the LLM to return the identified search queries in a structured object because we would like to use it for the next step in our workflow. Please take note of the following important points:\n",
        "\n",
        "1. We define a base `Queries` class that represents a list of the search strings returned by the LLM.\n",
        "2. We make use of the `with_structured_output` functionality as discussed in the lecture session to force the LLM to return a well formatted response that we can easily read into the object.\n",
        "3. After the search queries are returned, we make use TAVILY - a Search API provider to retrieve the results for our search term. This is the equivalent of performing Google searches yourself. For this you would need to have access to the TAVILY_API_KEY. You can sign-up for the Tavily API by clicking [here](https://app.tavily.com/sign-in) and follows the steps as shown in the screenshot."
      ],
      "metadata": {
        "id": "rJ_1qW_OqtuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from typing import TypedDict, Annotated, List\n",
        "\n",
        "class Queries(BaseModel):\n",
        "    queries: List[str]"
      ],
      "metadata": {
        "id": "Rh-7Ncop8cb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient\n",
        "import os\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
        "\n",
        "tavily = TavilyClient(api_key=userdata.get(\"TAVILY_API_KEY\"))\n",
        "\n",
        "def research_node(state: AgentState):\n",
        "  print (\"Research Agent executing ...\")\n",
        "  input_message = \"User query: \" + state['user_query']\n",
        "  if (state['review_comments'] is not None and state['review_comments'] != \"\"):\n",
        "    input_message = input_message + \"\\n Review Comments: \" + state['review_comments']\n",
        "  node_search_queries = model.with_structured_output(Queries).invoke([\n",
        "        SystemMessage(content=RESEARCH_PROMPT),\n",
        "        HumanMessage(content=input_message)\n",
        "    ])\n",
        "  search_queries = state['search_queries'] or []\n",
        "  search_queries.append(node_search_queries)\n",
        "  state['search_queries'] = search_queries\n",
        "  search_results = state['search_results'] or []\n",
        "  for q in node_search_queries.queries:\n",
        "    response = tavily.search(query=q, max_results=1)\n",
        "    for r in response['results']:\n",
        "      search_results.append(r['content'])\n",
        "  return {\"search_results\": search_results}"
      ],
      "metadata": {
        "id": "E7Rlop-38lFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will notice that we have used a function parameter `AgentState` where we store the search queries as well as the search results. This is the way in which Langraph allows us to do state management. Since we are designing workflows, it's important that outputs generated by one node should actually be available for subsequent node to work on or use. In this case, the research node will give the search results that the writer node needs in order to create the draft. In langraph, we achieve this by storing the values in the `AgentState` class. We can control which elements should be part of the state and keep adapting it as we add new nodes and operations to our workflow."
      ],
      "metadata": {
        "id": "sJTVdO0BsgY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, List\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    user_query: str\n",
        "    search_queries: List[str]\n",
        "    search_results: List[str]\n",
        "    draft_report: str\n",
        "    review_comments: str\n",
        "    draft_number: int\n",
        "    max_reviews: int"
      ],
      "metadata": {
        "id": "GBc2fYnl9jZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we find the need to create other state variables then we can come back and adapt it. In the next step, let's create the prompt that will generate the search queries."
      ],
      "metadata": {
        "id": "cOcYCg6o9doL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `research_node`, we have actually completed both Step1 and Step2 of the sub-tasks that we had outlined before. Let's move onto the draft generation step - the goal of this sub-task is to generate a detailed research report based on the retrieved search results.\n",
        "\n",
        "We create the LLM prompt for writing the article and as context we have to provide the search information that has been retrieved from the previous step. This is retrieved from the `AgentState` where we had already stored it. In addition, we add a variable to determine the number of times a draft is generated. We will make use of this later to control the flow. We initialize the variable `revision_number` to 1 once we have written the first draft. Each subsequent call of this function will also augment the revision_number by one."
      ],
      "metadata": {
        "id": "RK6mNCWlAbwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WRITER_PROMPT = \"\"\"You are an expert web assistant who can easily parse out nuggets of valuable information from\n",
        "mutliple web pages containing information.\n",
        "Along with the user task or problem, you are also provided a lot of information from crawled web pages and you\n",
        "have to generate the best research report that can complete the user task or offer a solution to the problem.\n",
        "Your report has to be insightful and provide the user with information that they were not aware of before.\n",
        "Your job is to make the life of the user easier so try to give answers rather than options.\n",
        "Utilize all the information below as needed:\n",
        "\n",
        "------\n",
        "\n",
        "{content}\"\"\""
      ],
      "metadata": {
        "id": "bJeTgtSYAopV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writer_node(state: AgentState):\n",
        "    print (\"Writing Agent executing ...\")\n",
        "    search_information = \"\\n\\n\".join(state['search_results'] or [])\n",
        "    user_message = HumanMessage(content=f\"The user query is - {state['user_query']}\")\n",
        "    messages = [SystemMessage(content=WRITER_PROMPT.format(content=search_information)),\n",
        "                user_message]\n",
        "    response = model.invoke(messages)\n",
        "    draft_number = 0\n",
        "    if (state.get(\"draft_number\") is not None):\n",
        "      draft_number = state.get(\"draft_number\")\n",
        "    return {\n",
        "        \"draft_report\": response.content,\n",
        "        \"draft_number\": draft_number + 1\n",
        "    }"
      ],
      "metadata": {
        "id": "ZilMy1V6As9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we store the drafted content in the state variable called `draft_report` and additionally also store the `draft_number`. This draft_report is what will be used in our next step."
      ],
      "metadata": {
        "id": "_3KqmEdhwElm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have defined the research and writer sub-tasks already and now it's time to define the review sub-task. We would certainly do this when writing a report ourselves but probably not so early in the process. Typically, we would prefer to write a more complete first version before having it reviewed - but each of our styles could also be different!\n",
        "\n",
        "However, when working with LLMs and agentic workflows, one of the common design patterns is to use reflection, or self-reflection. The idea behind it is that LLMs are not great at generating the best output in a start to finish manner. However, when asked to critique their own work, they are able to identify potential flaws and are capable of adapting the output to become better.\n",
        "\n",
        "Let's go ahead and define the reflection prompt and create the node."
      ],
      "metadata": {
        "id": "Eu-koN2uCPJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REFLECTION_PROMPT = \"\"\"Assume that you are a senior leader in an established technology company in the USA. You have requested \\\n",
        "your virtual assistant or intern to provide you with a short report or write-up based on a task or problem \\\n",
        "they are provided. The VA/Intern has done some web search and collated those results into a report.\n",
        "\n",
        "Please evaluate whether the report correctly answers the user task or problem. Think through your critique step by step and\n",
        "finally respond with very concise instructions on what additional information or aspects that you would like to have improved in the draft article.\"\"\""
      ],
      "metadata": {
        "id": "9Bvwk3JuDBPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reflection_node(state: AgentState):\n",
        "    print (\"Review Agent executing ...\")\n",
        "    messages = [\n",
        "        SystemMessage(content=REFLECTION_PROMPT),\n",
        "        HumanMessage(content=state['draft_report'])\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"review_comments\": response.content}"
      ],
      "metadata": {
        "id": "8TAEMT2-zeta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have created the various sub-tasks - research, draft generation and review/ reflection. The next step is to design our flow and put in the place the necessary feedback loops.\n",
        "\n",
        "Let's start by defining our graph and adding the nodes."
      ],
      "metadata": {
        "id": "l2Rt4SDwFP-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "research_graph = StateGraph(AgentState)\n",
        "research_graph.add_node(\"researcher\", research_node)\n",
        "research_graph.add_node(\"writer\", writer_node)\n",
        "research_graph.add_node(\"reflect\", reflection_node)"
      ],
      "metadata": {
        "id": "XKmDSqBHF-fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to define the workflow that we want. It's clear that our graph should start by doing the research, then use the results to write the first draft and then review it. This is what we define in our code -"
      ],
      "metadata": {
        "id": "pqwoDdvd0NDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "research_graph.set_entry_point(\"researcher\")\n",
        "research_graph.add_edge(\"researcher\", \"writer\")\n",
        "research_graph.add_edge(\"writer\", \"reflect\")\n",
        "research_graph.add_edge(\"reflect\", \"researcher\")"
      ],
      "metadata": {
        "id": "LN5cL--YHOk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we put all of this together and compile it to make sure that we have correctly defined the graph."
      ],
      "metadata": {
        "id": "0hkjRxHn00nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "researcher_agent = research_graph.compile()"
      ],
      "metadata": {
        "id": "7SD_gYoC0ypO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualize our graph by printing what it looks like."
      ],
      "metadata": {
        "id": "LKeksQRk07bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(researcher_agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "39Iw_XPU1AaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the issues that you might notice in the graph as we have defined it is that it is a continuous loop. There is no defined logic or clause at which we can say that we are done and happy with the article. So if we try to run it as is, we will end up with a non-terminating workflow and a lot of LLM API costs!\n",
        "\n",
        "Let's adapt our workflow to add a conditional edge. There are various ways in which we can define the conditional edge but let's start with a simple one that is purely based on the number of draft_version. If you remember, we have already tracked this number in the state and we can use that to create a conditional edge between the writer and reflect nodes.\n",
        "\n",
        "We start by specifying a specific function called `agent_goal` to determine whether the agent has acieved it's goal. We will pass in a new variable called `max_reviews` that will be checked against the draft_number. If the maximm number of iterations has been achieved, then we say that agent goal has been achieved and do not continue further."
      ],
      "metadata": {
        "id": "dwqIbGsU1CXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_goal(state: AgentState):\n",
        "  draft_number = state[\"draft_number\"]\n",
        "  max_reviews = state[\"max_reviews\"]\n",
        "  if draft_number > max_reviews:\n",
        "    print (\"End of Agent Execution\")\n",
        "    return \"AGENT_END\"\n",
        "  else:\n",
        "    print (\"Continue Agent Execution\")\n",
        "    return \"AGENT_CONTINUE\""
      ],
      "metadata": {
        "id": "8NSM-dJS1-2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to re-initilize our graph to take into account this new conditional edge. We replace the direct edge that we had between the writer and reflect nodes with the additional agent_goal."
      ],
      "metadata": {
        "id": "dOpbtrbs3SOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "research_graph = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "research_graph.add_node(\"researcher\", research_node)\n",
        "research_graph.add_node(\"writer\", writer_node)\n",
        "research_graph.add_node(\"reflect\", reflection_node)\n",
        "\n",
        "# Add edges\n",
        "research_graph.set_entry_point(\"researcher\")\n",
        "research_graph.add_edge(\"researcher\", \"writer\")\n",
        "research_graph.add_conditional_edges(\"writer\", agent_goal, {\"AGENT_END\": END, \"AGENT_CONTINUE\":\"reflect\"},)\n",
        "research_graph.add_edge(\"reflect\", \"researcher\")"
      ],
      "metadata": {
        "id": "ev58jr-J1xxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "researcher_agent = research_graph.compile()\n",
        "display(Image(researcher_agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "Q25Rw-7x2S1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will observe that the new graph doesn't appear like a loop anymore and has a defined end point based on the AGENT_END and AGENT_CONTINUE conditions.\n",
        "\n",
        "Let's try running the graph now for a maximum of two iterations to make sure that everything works."
      ],
      "metadata": {
        "id": "XrA9-EULz5Zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Running our agent"
      ],
      "metadata": {
        "id": "_zb-iWlBIvMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's call the agent now with our user question and see what the final report looks like. Particularly, we choose to stream the responses because we would like to be updated about what the agent is doing."
      ],
      "metadata": {
        "id": "zwWUUFS8JuQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGSMITH_TRACING'] = 'true'\n",
        "os.environ['LANGSMITH_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ['LANGSMITH_PROJECT'] = \"YOUR_PROJECT_NAME\""
      ],
      "metadata": {
        "id": "OJ47APrgAcsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = researcher_agent.invoke({'user_query': \"what is the back story or hidden secrets behind the internet website WebMD?\",\n",
        "                                  \"max_reviews\": 2,})"
      ],
      "metadata": {
        "id": "PuNGCCVjA0N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "ykpqZeJ9pYKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Adapting our agent\n",
        "\n",
        "Let's start playing around with our agent and give it some additional capabilities. We started by specifying the maximum number of revisions that we want the agent to do. Instead, what if we wanted to present some of these revisions to a human being and let them decide if they would like further revisions."
      ],
      "metadata": {
        "id": "qPn-vKmLAtQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do this, we add an explicit interrupt to the workflow. Typically, this is specified using the `interrup_before` command. In this case, we don't want the reflection step to happen unless the human user wants it to happen. We only need to compile a different version of our graph with `interrupt_before`."
      ],
      "metadata": {
        "id": "JFpMAnLQqFcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "Please adapt the `AgentState` class to accomodate the new functionality that we would like to add. We want to rely on a `human_review` to decide whether to continue with a review process and not based on a certain maximum number of reviews."
      ],
      "metadata": {
        "id": "oKza4FlpCn4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, List\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  ## Please decide which state variables you need to track"
      ],
      "metadata": {
        "id": "WYDDiTfLKTe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "In the next step, we need to adapt the `reflection_node` to perform an LLM-based review only when our user asks for it. In some cases, the user might also want to provide their own comments or feedback based on which the agent should adapt the behaviour. Please adapt the `reflection_node` function to implement this new functionality.\n",
        "\n",
        "Note: this depends on the `interrupt` step that we will add to the graph which we discuss in the project kick-off and in the next section, so please take that into account."
      ],
      "metadata": {
        "id": "lCpL8dEADJAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt the reflection_node to not make LLM calls if the human review has already been provided\n",
        "\n",
        "def reflection_node(state: AgentState):\n",
        "  ## Your code here"
      ],
      "metadata": {
        "id": "Ad92JteCantH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go ahead with the graph definition. We call this a new graph `human_research_graph` to indicate the human-in-the-loop aspect. You can see that we have the same nodes as before and the only change is that we do not have the conditional edge any longer. The reason for this is because we will add the `interrupt_before` which will replace the human instead of the previous conditional element."
      ],
      "metadata": {
        "id": "QK4FOOB_QUjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "human_research_graph = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "human_research_graph.add_node(\"researcher\", research_node)\n",
        "human_research_graph.add_node(\"writer\", writer_node)\n",
        "human_research_graph.add_node(\"reflect\", reflection_node)\n",
        "\n",
        "# Add edges\n",
        "human_research_graph.set_entry_point(\"researcher\")\n",
        "human_research_graph.add_edge(\"researcher\", \"writer\")\n",
        "human_research_graph.add_edge(\"writer\", \"reflect\")\n",
        "human_research_graph.add_edge(\"reflect\", \"researcher\")"
      ],
      "metadata": {
        "id": "aeXIEFb_LQ6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go ahead and compile the graph. There are two important things you will notice here:\n",
        "\n",
        "1. `interrupt_before` - this takes existing node or nodes as argument and will forecefully interrupt the execution of the graph before that node is reached. The resumption of the graph processing will have to be handled by our program logic. In this case, we define the interrup before 'reflect' node as we want to check with the user what they would like to do.\n",
        "2. `checkpointer` - this is an additional argument that is required when working with interrupts and also to take care of multiple states that a graph might have. When the graph is interrupted, it must also save all the information in it's current state which can be pulled back when required. The checkpointer saves this state information in a local SQLITE database that is running in memory. In addition, you can have multiple users of the app each of whom might pause graph execution. The state variables must also not be mixed up and that's another reason why having the checkpointer is essential."
      ],
      "metadata": {
        "id": "9divKxF_QwkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "mod_researcher_agent = human_research_graph.compile(checkpointer=memory,\n",
        "                                                    interrupt_before=[\"reflect\"])\n",
        "\n",
        "display(Image(mod_researcher_agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "b903HdfJL2Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try out the whole graph with the interrupt by running it directly before building the app."
      ],
      "metadata": {
        "id": "G2ppNli-R0zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = mod_researcher_agent.invoke({'user_query': \"what is the back story or hidden secrets behind the internet website WebMD?\",\n",
        "                                      \"max_reviews\": 2},\n",
        "                                     config = {\"configurable\": {\"thread_id\": 1}})"
      ],
      "metadata": {
        "id": "N-XzN48VMliC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will already see that the graph stops execution after the writing node and before the reflect node has started. An initial draft report would have been generated as shown below."
      ],
      "metadata": {
        "id": "a99_9wcTR_yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "display(Markdown(output['draft_report']))"
      ],
      "metadata": {
        "id": "D0rPolHGbo9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the point at which we need to identify logic for handling the interrupt. Let's hard-code it for now to see how it works. What we are doing in the following step is as follows:\n",
        "\n",
        "1. Get the configuration for the specific thread that we are interested in. `thread_id` is our current way to identify each new execution of the graph.\n",
        "2. We have access to the state variables from the checkpoint and can actually make any changes. In this case, please update the state variable that you used for the human_review."
      ],
      "metadata": {
        "id": "qKkJKsEbSJ71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Get the config based on the thread_id\n",
        "config = {\"configurable\": {\"thread_id\": 1}}\n",
        "\n",
        "## View the state variables\n",
        "mod_researcher_agent.get_state(config).values\n"
      ],
      "metadata": {
        "id": "gusbV2GMNVCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "Please update the state variable to pass in any human review comments"
      ],
      "metadata": {
        "id": "SqQSwR2KTHCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Make necessary changes to the state variable to pass in the human review comments\n",
        "# mod_researcher_agent.update_state(config, ...)\n",
        "\n",
        "## You can confirm that the update of state has happened\n",
        "# mod_researcher_agent.get_state(config).values"
      ],
      "metadata": {
        "id": "Lot0SlhWS-aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have made the necessary changes to the state, you can resume the graph processing by calling `invoke` again without passing any input but with the same config. This resumes the graph processing."
      ],
      "metadata": {
        "id": "m21U1rgkTaEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = mod_researcher_agent.invoke(None, config)"
      ],
      "metadata": {
        "id": "-199dLigOWt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will notice that the graph continued to execute and went back to the research and write nodes and then was interrupted again before the next call to the reflect node. So we are achieving the same functionality as before but with a human-in-the-loop intervention. You can view the adjusted draft article as shown below:"
      ],
      "metadata": {
        "id": "qqiKHDGHTpPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "display(Markdown(output['draft_report']))"
      ],
      "metadata": {
        "id": "B6c86AEPcagB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We ran the modified graph by simulating a human-in-the-loop interruption and manually passing in the new state values. Let's now integrate this into a product and see how it will behave with actual user interaction.\n",
        "\n",
        "## üìù Learner Task:\n",
        "\n",
        "Please create a Gradio front-end that incporporates the following functions:\n",
        "\n",
        "1. `research_report`: a function that triggers the first graph execution and returns the first version of the draft article.\n",
        "2. `do_human_review`: a function that takes human feedback and uses that to continue with the graph exection. It returns the next draft version of the article.\n",
        "3. `do_AI_review`: a function that requests an LLM to do a review and continues with the graph execution. It also returns the next draft version of the article but after an AI review.\n",
        "\n",
        "One critical aspect to take care of is creating the `thread_id` which is a new random integer uniquely used for each user request. We have already provided the necessary scaffolding for the gradio components and request you to complete the functions as defined above."
      ],
      "metadata": {
        "id": "oWGTA0FKqZud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a usable front-end for testing this new agent, which can also be exposed to potential users. We make use of Gradio to code this functionality but it could be any front-end platform."
      ],
      "metadata": {
        "id": "wIX1qpvYrWMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import gradio as gr\n",
        "\n",
        "mod_researcher_agent = human_research_graph.compile(checkpointer=memory,\n",
        "                                              interrupt_before=[\"reflect\"])\n",
        "\n",
        "def research_report(user_query, thread_id):\n",
        "  ## Create a new thread_id to track this request\n",
        "  agent_thread = random.randint(0, 1000)\n",
        "  ## Add it to the Gradio state variable which helps to maintain this throughout the session\n",
        "  thread_id.append(agent_thread)\n",
        "\n",
        "  ## Please complete the rest of the function where graph processing is initiated\n",
        "\n",
        "  return draft_report, thread_id\n",
        "\n",
        "def do_human_review(user_query, human_review, thread_id):\n",
        "  ## Retrieve the graph state based on the thread_id\n",
        "  config = {\"configurable\": {\"thread_id\": thread_id[0]}}\n",
        "\n",
        "  ## Please complete the rest of the function where you update the state variable with the human_review comment\n",
        "\n",
        "  return draft_report, thread_id\n",
        "\n",
        "def do_AI_review(user_query, thread_id):\n",
        "  ## Retrieve the graph state based on the thread_id\n",
        "  config = {\"configurable\": {\"thread_id\": thread_id[0]}}\n",
        "\n",
        "  ## Please complete the rest of the function where you update the state variable with the human_review comment\n",
        "\n",
        "  return draft_report, thread_id\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  gr.Markdown(\"# Research Agent\")\n",
        "  thread_id = gr.State([])\n",
        "  user_query = gr.Textbox(lines=2, placeholder=\"Enter your research query here...\")\n",
        "  research_btn = gr.Button(\"Generate Research Report\")\n",
        "\n",
        "  draft_report = gr.Textbox(lines=10)\n",
        "  AI_review_btn = gr.Button(\"This report is great, can you do an AI review?\")\n",
        "  human_review_btn = gr.Button(\"This report needs work, please use my comments below.\")\n",
        "  human_review = gr.Textbox(lines=1)\n",
        "\n",
        "  research_btn.click(research_report, inputs=[user_query, thread_id], outputs=[draft_report, thread_id])\n",
        "  human_review_btn.click(do_human_review, inputs=[user_query, human_review, thread_id], outputs=[draft_report, thread_id])\n",
        "  AI_review_btn.click(do_AI_review, inputs=[user_query, thread_id], outputs=[draft_report, thread_id])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "TBsxGXKrAsUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional Learner Tasks:"
      ],
      "metadata": {
        "id": "E808xQ3BYboV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Better reviewer\n",
        "\n",
        "In the graph above, we used `gpt-4o-mini` as the LLM for all of our tasks. However, it might make sense that we use a better (smarter?) LLM to perform the review of our article to get better feedback. Can you adapt the `reflection_node` to use a different LLM like say `gpt-4o` or `claude-sonnet-3.5`"
      ],
      "metadata": {
        "id": "d-5kVMrbW-DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-anthropic"
      ],
      "metadata": {
        "id": "VdfSPQasghbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "model_name = \"claude-3.5-sonnet\"\n",
        "reflection_llm = ChatAnthropic(\n",
        "    model=model_name,\n",
        "    api_key=userdata.get(\"CLAUDE_API_KEY\")\n",
        ")\n",
        "\n",
        "def reflection_node(state: AgentState):\n",
        "    ## Perform necessary adaptations to this function to make use of a different LLM"
      ],
      "metadata": {
        "id": "pzGbyWbGDVJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Different output formats\n",
        "\n",
        "In this graph, we focussed on generating an article. What if we could also consider other output formats like a LinkedIn post or a Twitter thread. Could you adapt the graph to add additional nodes that would use the draft article as input and generate a LinkedIn post or Twitter thread. Consider where would you add these nodes and what kind of new edges will be required."
      ],
      "metadata": {
        "id": "D8yN9gvEXm-_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWdIEXucW_vc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}